{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:09.437363Z",
     "iopub.status.busy": "2022-04-13T08:19:09.436065Z",
     "iopub.status.idle": "2022-04-13T08:19:11.746363Z",
     "shell.execute_reply": "2022-04-13T08:19:11.745518Z",
     "shell.execute_reply.started": "2022-04-13T08:19:09.437312Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 导入paddle库\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from paddle.io import DataLoader\n",
    "from paddle.dataset.common import md5file\n",
    "# 导入paddlenlp的库\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from paddlenlp.transformers import BertTokenizer,BertPretrainedModel\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\n",
    "from paddlenlp.datasets import DatasetBuilder,get_path_from_url\n",
    "# 导入所需要的py包\n",
    "from paddle.io import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:14.967564Z",
     "iopub.status.busy": "2022-04-13T08:19:14.967031Z",
     "iopub.status.idle": "2022-04-13T08:19:15.270488Z",
     "shell.execute_reply": "2022-04-13T08:19:15.269519Z",
     "shell.execute_reply.started": "2022-04-13T08:19:14.967526Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data110628/剧本角色情感识别.zip\n",
      "  inflating: data/submit_example.tsv  \n",
      "  inflating: data/__MACOSX/._submit_example.tsv  \n",
      "  inflating: data/test_dataset.tsv   \n",
      "  inflating: data/__MACOSX/._test_dataset.tsv  \n",
      "  inflating: data/train_dataset_v2.tsv  \n",
      "  inflating: data/__MACOSX/._train_dataset_v2.tsv  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/data110628/剧本角色情感识别.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:17.758512Z",
     "iopub.status.busy": "2022-04-13T08:19:17.758007Z",
     "iopub.status.idle": "2022-04-13T08:19:17.964947Z",
     "shell.execute_reply": "2022-04-13T08:19:17.963984Z",
     "shell.execute_reply.started": "2022-04-13T08:19:17.758473Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42790/42790 [00:00<00:00, 704489.23it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/train_dataset_v2.tsv', 'r', encoding='utf-8') as handler:\n",
    "    lines = handler.read().split('\\n')[1:-1]\n",
    "\n",
    "    data = list()\n",
    "    for line in tqdm(lines):\n",
    "        sp = line.split('\\t')\n",
    "        if len(sp) != 4:\n",
    "            print(\"ERROR:\", sp)\n",
    "            continue\n",
    "        data.append(sp)\n",
    "\n",
    "train = pd.DataFrame(data)\n",
    "train.columns = ['id', 'content', 'character', 'emotions']\n",
    "\n",
    "test = pd.read_csv('data/test_dataset.tsv', sep='\\t')\n",
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')\n",
    "train = train[train['emotions'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:19.465102Z",
     "iopub.status.busy": "2022-04-13T08:19:19.463889Z",
     "iopub.status.idle": "2022-04-13T08:19:20.290567Z",
     "shell.execute_reply": "2022-04-13T08:19:20.289445Z",
     "shell.execute_reply.started": "2022-04-13T08:19:19.465056Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['text'] = train[ 'content'].astype(str)  +'角色: ' + train['character'].astype(str)\n",
    "test['text'] = test['content'].astype(str) + ' 角色: ' + test['character'].astype(str)\n",
    "\n",
    "train['emotions'] = train['emotions'].apply(lambda x: [int(_i) for _i in x.split(',')])\n",
    "\n",
    "train[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] = train['emotions'].values.tolist()\n",
    "test[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] =[0,0,0,0,0,0]\n",
    "\n",
    "train.to_csv('data/train.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)\n",
    "\n",
    "test.to_csv('data/test.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:40.616173Z",
     "iopub.status.busy": "2022-04-13T08:19:40.615826Z",
     "iopub.status.idle": "2022-04-13T08:19:55.462931Z",
     "shell.execute_reply": "2022-04-13T08:19:55.462047Z",
     "shell.execute_reply.started": "2022-04-13T08:19:40.616142Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-13 16:19:40,619] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/roberta_base/vocab.txt and saved to /home/aistudio/.paddlenlp/models/roberta-wwm-ext\n",
      "[2022-04-13 16:19:40,622] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/roberta_base/vocab.txt\n",
      "100%|██████████| 107/107 [00:00<00:00, 1558.55it/s]\n",
      "[2022-04-13 16:19:40,800] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/roberta_base/roberta_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/roberta-wwm-ext\n",
      "[2022-04-13 16:19:40,805] [    INFO] - Downloading roberta_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/roberta_base/roberta_chn_base.pdparams\n",
      "100%|██████████| 399505/399505 [00:07<00:00, 51719.84it/s]\n",
      "W0413 16:19:48.635334   163 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0413 16:19:48.641345   163 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "target_cols=['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']\n",
    "# PRE_TRAINED_MODEL_NAME=\"bert-base-chinese\"\n",
    "# PRE_TRAINED_MODEL_NAME='macbert-base-chinese'\n",
    "\n",
    "# 读者可以在这里切换语言模型\n",
    "\n",
    "\n",
    "# 加载BERT的分词器\n",
    "# PRE_TRAINED_MODEL_NAME='macbert-large-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# base_model=ppnlp.transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# PRE_TRAINED_MODEL_NAME='bert-wwm-ext-chinese'\n",
    "# base_model = ppnlp.transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# roberta\n",
    "PRE_TRAINED_MODEL_NAME='roberta-wwm-ext'\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "base_model = ppnlp.transformers.RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)  # 加载预训练模型\n",
    "# model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:19:57.428883Z",
     "iopub.status.busy": "2022-04-13T08:19:57.427733Z",
     "iopub.status.idle": "2022-04-13T08:19:58.019804Z",
     "shell.execute_reply": "2022-04-13T08:19:58.019016Z",
     "shell.execute_reply.started": "2022-04-13T08:19:57.428836Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RoleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train',trans_func=None):\n",
    "\n",
    "        super(RoleDataset, self).__init__()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.data = pd.read_csv('data/train.csv',sep='\\t')\n",
    "        else:\n",
    "            self.data = pd.read_csv('data/test.csv',sep='\\t')\n",
    "        self.texts=self.data['text'].tolist()\n",
    "        self.labels=self.data[target_cols].to_dict('records')\n",
    "        self.trans_func=trans_func\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text=str(self.texts[index])\n",
    "        label=self.labels[index]\n",
    "        sample = {\n",
    "            'text': text\n",
    "        }\n",
    "        for label_col in target_cols:\n",
    "            sample[label_col] =label[label_col]\n",
    "        sample=self.trans_func(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "# 转换成id的函数\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    # print(example)\n",
    "    sample={}\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "    sample['input_ids'] = encoded_inputs[\"input_ids\"]\n",
    "    sample['token_type_ids'] = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    sample['love'] = np.array(example[\"love\"], dtype=\"float32\")\n",
    "    sample['joy'] = np.array(example[\"joy\"], dtype=\"float32\")\n",
    "    sample['anger'] = np.array(example[\"anger\"], dtype=\"float32\")\n",
    "\n",
    "    sample['fright'] = np.array(example[\"fright\"], dtype=\"float32\")\n",
    "    sample['fear'] = np.array(example[\"fear\"], dtype=\"float32\")\n",
    "    sample['sorrow'] = np.array(example[\"sorrow\"], dtype=\"float32\")\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "max_seq_length=128\n",
    "trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "train_ds=RoleDataset('train',trans_func)\n",
    "test_ds=RoleDataset('test',trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:01.702599Z",
     "iopub.status.busy": "2022-04-13T08:20:01.702071Z",
     "iopub.status.idle": "2022-04-13T08:20:01.711885Z",
     "shell.execute_reply": "2022-04-13T08:20:01.710692Z",
     "shell.execute_reply.started": "2022-04-13T08:20:01.702554Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4959, 4708, 5520, 2552, 4638, 9338, 7008, 3341, 8024, 4692, 4692, 2797, 3322, 8024, 676, 4157, 749, 511, 6235, 5682, 131, 9338, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'love': array(0., dtype=float32), 'joy': array(0., dtype=float32), 'anger': array(0., dtype=float32), 'fright': array(0., dtype=float32), 'fear': array(0., dtype=float32), 'sorrow': array(0., dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:08.901415Z",
     "iopub.status.busy": "2022-04-13T08:20:08.900339Z",
     "iopub.status.idle": "2022-04-13T08:20:08.908092Z",
     "shell.execute_reply": "2022-04-13T08:20:08.906592Z",
     "shell.execute_reply.started": "2022-04-13T08:20:08.901343Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=3\n",
    "weight_decay=0.0\n",
    "data_path='data'\n",
    "warmup_proportion=0.0\n",
    "init_from_ckpt=None\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "learning_rate=5e-5\n",
    "\n",
    "\n",
    "# # 把训练集合转换成id\n",
    "# train_ds = train_ds.map(partial(convert_example, tokenizer=tokenizer))\n",
    "\n",
    "# # 构建训练集合的dataloader\n",
    "# train_batch_sampler = paddle.io.BatchSampler(dataset=train_ds, batch_size=32, shuffle=True)\n",
    "# train_data_loader = paddle.io.DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:13.769369Z",
     "iopub.status.busy": "2022-04-13T08:20:13.768861Z",
     "iopub.status.idle": "2022-04-13T08:20:13.774712Z",
     "shell.execute_reply": "2022-04-13T08:20:13.774063Z",
     "shell.execute_reply.started": "2022-04-13T08:20:13.769333Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:15.676098Z",
     "iopub.status.busy": "2022-04-13T08:20:15.674874Z",
     "iopub.status.idle": "2022-04-13T08:20:15.686564Z",
     "shell.execute_reply": "2022-04-13T08:20:15.685249Z",
     "shell.execute_reply.started": "2022-04-13T08:20:15.676035Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_func(batch_data):\n",
    "    # 获取batch数据的大小\n",
    "    batch_size = len(batch_data)\n",
    "    # 如果batch_size为0，则返回一个空字典\n",
    "    if batch_size == 0:\n",
    "        return {}\n",
    "    input_ids_list, attention_mask_list = [], []\n",
    "    love_list,joy_list,anger_list=[],[],[]\n",
    "    fright_list,fear_list,sorrow_list=[],[],[]\n",
    "    # 遍历batch数据，将每一个数据，转换成tensor的形式\n",
    "    for instance in batch_data:\n",
    "        input_ids_temp = instance[\"input_ids\"]\n",
    "        attention_mask_temp = instance[\"token_type_ids\"]\n",
    "\n",
    "        love=instance['love'] \n",
    "        joy=instance['joy'] \n",
    "        anger=instance['anger'] \n",
    "\n",
    "        fright= instance['fright'] \n",
    "        fear=instance['fear'] \n",
    "        sorrow=instance['sorrow'] \n",
    "\n",
    "        input_ids_list.append(paddle.to_tensor(input_ids_temp, dtype=\"int64\"))\n",
    "        attention_mask_list.append(paddle.to_tensor(attention_mask_temp, dtype=\"int64\"))\n",
    "\n",
    "        love_list.append(love)\n",
    "        joy_list.append(joy)\n",
    "        anger_list.append(anger)\n",
    "\n",
    "        fright_list.append(fright)\n",
    "        fear_list.append(fear)\n",
    "        sorrow_list.append(sorrow)\n",
    "\n",
    "    # 对一个batch内的数据，进行padding\n",
    "    return {\"input_ids\": Pad(pad_val=0, axis=0)(input_ids_list),\n",
    "            \"token_type_ids\": Pad(pad_val=0, axis=0)(attention_mask_list),\n",
    "            \"love\": Stack(dtype=\"int64\")(love_list),\n",
    "            \"joy\": Stack(dtype=\"int64\")(joy_list),\n",
    "            \"anger\": Stack(dtype=\"int64\")(anger_list),\n",
    "            \"fright\": Stack(dtype=\"int64\")(fright_list),\n",
    "            \"fear\": Stack(dtype=\"int64\")(fear_list),\n",
    "            \"sorrow\": Stack(dtype=\"int64\")(sorrow_list),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:17.460116Z",
     "iopub.status.busy": "2022-04-13T08:20:17.459548Z",
     "iopub.status.idle": "2022-04-13T08:20:17.464964Z",
     "shell.execute_reply": "2022-04-13T08:20:17.464126Z",
     "shell.execute_reply.started": "2022-04-13T08:20:17.460075Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=collate_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:19.726112Z",
     "iopub.status.busy": "2022-04-13T08:20:19.725727Z",
     "iopub.status.idle": "2022-04-13T08:20:19.746348Z",
     "shell.execute_reply": "2022-04-13T08:20:19.745368Z",
     "shell.execute_reply.started": "2022-04-13T08:20:19.726077Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Layer):\n",
    "    def __init__(self, bert,n_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.out_love = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "        self.out_joy = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "        self.out_fright = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "        self.out_anger = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "        self.out_fear = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "        self.out_sorrow = nn.Linear(self.bert.config[\"hidden_size\"], n_classes)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        love = self.out_love(pooled_output)\n",
    "        joy = self.out_joy(pooled_output)\n",
    "        fright = self.out_fright(pooled_output)\n",
    "        anger = self.out_anger(pooled_output)\n",
    "        fear = self.out_fear(pooled_output)\n",
    "        sorrow = self.out_sorrow(pooled_output)\n",
    "        return {\n",
    "            'love': love, 'joy': joy, 'fright': fright,\n",
    "            'anger': anger, 'fear': fear, 'sorrow': sorrow,\n",
    "        }\n",
    "\n",
    "class_names=[1]\n",
    "model = EmotionClassifier(base_model,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:20:27.161212Z",
     "iopub.status.busy": "2022-04-13T08:20:27.160640Z",
     "iopub.status.idle": "2022-04-13T08:20:27.169848Z",
     "shell.execute_reply": "2022-04-13T08:20:27.169176Z",
     "shell.execute_reply.started": "2022-04-13T08:20:27.161161Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_train_epochs=3\n",
    "num_training_steps = len(train_data_loader) * num_train_epochs\n",
    "\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:24:13.656329Z",
     "iopub.status.busy": "2022-04-13T08:24:13.655205Z",
     "iopub.status.idle": "2022-04-13T08:34:38.639594Z",
     "shell.execute_reply": "2022-04-13T08:34:38.638777Z",
     "shell.execute_reply.started": "2022-04-13T08:24:13.656282Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 0, batch: 99, loss: 2.75459, accuracy: 0.90583, speed: 5.66 step/s\n",
      "global step 200, epoch: 0, batch: 199, loss: 1.56995, accuracy: 0.90971, speed: 2.83 step/s\n",
      "global step 300, epoch: 0, batch: 299, loss: 1.81763, accuracy: 0.91165, speed: 1.89 step/s\n",
      "global step 400, epoch: 0, batch: 399, loss: 1.46230, accuracy: 0.91280, speed: 1.41 step/s\n",
      "global step 500, epoch: 0, batch: 499, loss: 1.36762, accuracy: 0.91340, speed: 1.13 step/s\n",
      "global step 600, epoch: 0, batch: 599, loss: 1.39702, accuracy: 0.91343, speed: 0.93 step/s\n",
      "global step 700, epoch: 0, batch: 699, loss: 1.63529, accuracy: 0.91405, speed: 0.80 step/s\n",
      "global step 800, epoch: 0, batch: 799, loss: 1.49754, accuracy: 0.91436, speed: 0.70 step/s\n",
      "global step 900, epoch: 0, batch: 899, loss: 2.63329, accuracy: 0.91436, speed: 0.62 step/s\n",
      "global step 1000, epoch: 0, batch: 999, loss: 2.00582, accuracy: 0.91466, speed: 0.56 step/s\n",
      "global step 1100, epoch: 0, batch: 1099, loss: 1.56080, accuracy: 0.91471, speed: 0.51 step/s\n",
      "global step 1200, epoch: 1, batch: 49, loss: 1.64822, accuracy: 0.92344, speed: 0.46 step/s\n",
      "global step 1300, epoch: 1, batch: 149, loss: 1.72939, accuracy: 0.91910, speed: 0.43 step/s\n",
      "global step 1400, epoch: 1, batch: 249, loss: 1.43186, accuracy: 0.91908, speed: 0.40 step/s\n",
      "global step 1500, epoch: 1, batch: 349, loss: 1.49311, accuracy: 0.91951, speed: 0.37 step/s\n",
      "global step 1600, epoch: 1, batch: 449, loss: 1.80245, accuracy: 0.91963, speed: 0.35 step/s\n",
      "global step 1700, epoch: 1, batch: 549, loss: 1.23721, accuracy: 0.92020, speed: 0.33 step/s\n",
      "global step 1800, epoch: 1, batch: 649, loss: 1.08680, accuracy: 0.92089, speed: 0.31 step/s\n",
      "global step 1900, epoch: 1, batch: 749, loss: 1.31149, accuracy: 0.92101, speed: 0.29 step/s\n",
      "global step 2000, epoch: 1, batch: 849, loss: 0.91808, accuracy: 0.92115, speed: 0.28 step/s\n",
      "global step 2100, epoch: 1, batch: 949, loss: 1.03612, accuracy: 0.92177, speed: 0.26 step/s\n",
      "global step 2200, epoch: 1, batch: 1049, loss: 1.35397, accuracy: 0.92190, speed: 0.25 step/s\n",
      "global step 2300, epoch: 1, batch: 1149, loss: 1.78144, accuracy: 0.92210, speed: 0.24 step/s\n",
      "global step 2400, epoch: 2, batch: 99, loss: 1.10440, accuracy: 0.93474, speed: 0.23 step/s\n",
      "global step 2500, epoch: 2, batch: 199, loss: 1.62714, accuracy: 0.93490, speed: 0.22 step/s\n",
      "global step 2600, epoch: 2, batch: 299, loss: 1.16805, accuracy: 0.93500, speed: 0.21 step/s\n",
      "global step 2700, epoch: 2, batch: 399, loss: 0.76830, accuracy: 0.93530, speed: 0.20 step/s\n",
      "global step 2800, epoch: 2, batch: 499, loss: 0.88295, accuracy: 0.93552, speed: 0.20 step/s\n",
      "global step 2900, epoch: 2, batch: 599, loss: 0.64697, accuracy: 0.93540, speed: 0.19 step/s\n",
      "global step 3000, epoch: 2, batch: 699, loss: 0.99520, accuracy: 0.93510, speed: 0.18 step/s\n",
      "global step 3100, epoch: 2, batch: 799, loss: 1.70965, accuracy: 0.93540, speed: 0.18 step/s\n",
      "global step 3200, epoch: 2, batch: 899, loss: 1.38639, accuracy: 0.93579, speed: 0.17 step/s\n",
      "global step 3300, epoch: 2, batch: 999, loss: 1.03550, accuracy: 0.93564, speed: 0.17 step/s\n",
      "global step 3400, epoch: 2, batch: 1099, loss: 1.18741, accuracy: 0.93597, speed: 0.16 step/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1591964"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_train( model, data_loader,  criterion,  optimizer, scheduler,  metric ):\n",
    "    \n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    log_steps=100\n",
    "    for epoch in range(num_train_epochs):\n",
    "        losses = []\n",
    "        # optimizer .zero_gard()\n",
    "        for step,sample in enumerate(data_loader):\n",
    "            # print(sample)\n",
    "            input_ids = sample[\"input_ids\"]\n",
    "            token_type_ids = sample[\"token_type_ids\"]\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids)\n",
    "            # print(outputs)\n",
    "\n",
    "            loss_love = criterion(outputs['love'], sample['love'])\n",
    "            loss_joy = criterion(outputs['joy'], sample['joy'])\n",
    "            loss_fright = criterion(outputs['fright'], sample['fright'])\n",
    "\n",
    "            loss_anger = criterion(outputs['anger'], sample['anger'])\n",
    "            loss_fear = criterion(outputs['fear'], sample['fear'])\n",
    "            loss_sorrow = criterion(outputs['sorrow'], sample['sorrow'])\n",
    "\n",
    "            loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow\n",
    "\n",
    "            for label_col in target_cols:\n",
    "                correct = metric.compute(outputs[label_col], sample[label_col])\n",
    "                metric.update(correct)\n",
    "\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            losses.append(loss.numpy())\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            global_step += 1\n",
    "            # 每间隔 log_steps 输出训练指标\n",
    "            if global_step % log_steps == 0:\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, accuracy: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    log_steps / (time.time() - tic_train)))\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "        metric.reset()\n",
    "    return np.mean(losses)\n",
    "\n",
    "do_train(model,train_data_loader,criterion,optimizer,lr_scheduler,metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:35:55.191849Z",
     "iopub.status.busy": "2022-04-13T08:35:55.191235Z",
     "iopub.status.idle": "2022-04-13T08:35:55.363964Z",
     "shell.execute_reply": "2022-04-13T08:35:55.350684Z",
     "shell.execute_reply.started": "2022-04-13T08:35:55.191805Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'love': [array([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)], 'joy': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)], 'fright': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)], 'anger': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)], 'fear': [array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)], 'sorrow': [array([0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)]})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_data_loader = create_dataloader(\n",
    "        test_ds,\n",
    "        mode='test',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=collate_func)\n",
    "        \n",
    "test_pred = defaultdict(list)\n",
    "for step, batch in tqdm(enumerate(test_data_loader)):\n",
    "    b_input_ids = batch['input_ids']\n",
    "    token_type_ids = batch['token_type_ids']\n",
    "    logits = model(input_ids=b_input_ids, token_type_ids=token_type_ids)\n",
    "    for col in target_cols:\n",
    "        out2 = paddle.argmax(logits[col], axis=1)\n",
    "        test_pred[col].append(out2.numpy())\n",
    "    print(test_pred)\n",
    "    # print(logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:36:47.553794Z",
     "iopub.status.busy": "2022-04-13T08:36:47.553277Z",
     "iopub.status.idle": "2022-04-13T08:37:32.161286Z",
     "shell.execute_reply": "2022-04-13T08:37:32.160502Z",
     "shell.execute_reply.started": "2022-04-13T08:36:47.553758Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "668it [00:44, 15.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(model,test_data_loader):\n",
    "    val_loss = 0\n",
    "    test_pred = defaultdict(list)\n",
    "    model.eval()\n",
    "    for step, batch in tqdm(enumerate(test_data_loader)):\n",
    "        b_input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "\n",
    "        with paddle.no_grad():\n",
    "            logits = model(input_ids=b_input_ids, token_type_ids=token_type_ids)\n",
    "            for col in target_cols:\n",
    "                out2 = paddle.argmax(logits[col], axis=1)\n",
    "                test_pred[col].extend(out2.numpy().tolist())\n",
    "    return test_pred\n",
    "\n",
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')\n",
    "test_pred = predict(model,test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:37:32.184400Z",
     "iopub.status.busy": "2022-04-13T08:37:32.183850Z",
     "iopub.status.idle": "2022-04-13T08:37:32.193886Z",
     "shell.execute_reply": "2022-04-13T08:37:32.193042Z",
     "shell.execute_reply.started": "2022-04-13T08:37:32.184346Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
      "21376\n"
     ]
    }
   ],
   "source": [
    "print(test_pred['love'][:10])\n",
    "print(len(test_pred['love']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T08:37:36.683534Z",
     "iopub.status.busy": "2022-04-13T08:37:36.682685Z",
     "iopub.status.idle": "2022-04-13T08:37:36.789266Z",
     "shell.execute_reply": "2022-04-13T08:37:36.788096Z",
     "shell.execute_reply.started": "2022-04-13T08:37:36.683465Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34170_0002_A_12</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34170_0002_A_14</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34170_0003_A_16</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34170_0003_A_17</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34170_0003_A_18</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id      emotion\n",
       "0  34170_0002_A_12  0,0,0,0,0,0\n",
       "1  34170_0002_A_14  0,0,0,0,0,0\n",
       "2  34170_0003_A_16  0,0,0,0,0,0\n",
       "3  34170_0003_A_17  0,0,0,0,0,0\n",
       "4  34170_0003_A_18  0,0,0,0,0,0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_preds = []\n",
    "for col in target_cols:\n",
    "    preds = test_pred[col]\n",
    "    label_preds.append(preds)\n",
    "print(len(label_preds[0]))\n",
    "sub = submit.copy()\n",
    "sub['emotion'] = np.stack(label_preds, axis=1).tolist()\n",
    "sub['emotion'] = sub['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "sub.to_csv('baseline_{}.tsv'.format(PRE_TRAINED_MODEL_NAME), sep='\\t', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
